{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b54ce88c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from dotenv import load_dotenv\n",
    "\n",
    "load_dotenv()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "1034cef7",
   "metadata": {},
   "outputs": [],
   "source": [
    "OPEN_API_KEY = os.getenv('OPEN_API_KEY')\n",
    "\n",
    "LANGSMITH_TRACING = os.getenv('LANGSMITH_TRACING')\n",
    "LANGSMITH_API_KEY = os.getenv('LANGSMITH_API_KEY')\n",
    "LANGSMITH_PROJECT = os.getenv('LANGSMITH_PROJECT')\n",
    "LANGSMITH_ENDPOINT = os.getenv('LANGSMITH_ENDPOINT')\n",
    "\n",
    "TAVILY_API_KEY = os.getenv('TAVILY_API_KEY')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ff44dec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_openai import ChatOpenAI\n",
    "\n",
    "llm = ChatOpenAI(model=\"gpt-4o\", temperature=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "e1001c61",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tavily import TavilyClient\n",
    "\n",
    "# Initialize Tavily client\n",
    "tavily_client = TavilyClient(api_key=TAVILY_API_KEY)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "431ad60a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pydantic import BaseModel, Field\n",
    "\n",
    "\n",
    "from typing import TypedDict, List\n",
    "from typing import Annotated\n",
    "from operator import add\n",
    "from typing import Literal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "61e28e1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class TavilySearchOutput(BaseModel):\n",
    "    title: str\n",
    "    url: str\n",
    "    content: str\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "fd1c54ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "class AgentState(TypedDict):\n",
    "    query: str\n",
    "    tavilySearchOutput: List[TavilySearchOutput]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "20cfcbc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This function takes your search term, looks it up on three dataset-heavy sites, throws away useless or duplicate links, and gives you back a clean list of dataset pages you can actually use.\n",
    "\n",
    "def tavily_search_tool(state: AgentState):\n",
    "    \"\"\"Search Kaggle + Hugging Face and return up to 10 most relevant, mixed, deduped dataset links.\"\"\"\n",
    "    query = state[\"query\"]\n",
    "\n",
    "    site_queries = {\n",
    "        \"kaggle\": f\"{query} dataset site:kaggle.com/datasets\",\n",
    "        \"huggingface\": f\"{query} dataset site:huggingface.co/datasets\"\n",
    "    }\n",
    "\n",
    "    def which_site(url: str) -> str | None:\n",
    "        u = url.lower()\n",
    "        if \"kaggle.com/datasets/\" in u:\n",
    "            return \"kaggle\"\n",
    "        if \"huggingface.co/datasets/\" in u:\n",
    "            return \"huggingface\"\n",
    "        return None\n",
    "\n",
    "    def relevance_score(site: str, title: str, url: str, content: str) -> float:\n",
    "        t = (title or \"\").lower()\n",
    "        u = (url or \"\").lower()\n",
    "        c = (content or \"\").lower()\n",
    "        score = 0.0\n",
    "        # Strong signals of downloadable/data-bearing resources\n",
    "        if any(x in u for x in [\".csv\", \".xlsx\", \".json\", \"/download\", \"/resolve/\", \"/raw/\"]):\n",
    "            score += 3.0\n",
    "        if any(x in c for x in [\"csv\", \"xlsx\", \"json\", \"download\"]):\n",
    "            score += 2.0\n",
    "        # Query keyword overlap (simple)\n",
    "        for tok in set(query.lower().split()):\n",
    "            if tok and (tok in t or tok in c):\n",
    "                score += 0.5\n",
    "        # Site priors (dataset-centric sites get a small boost)\n",
    "        if site in (\"kaggle\", \"huggingface\"):\n",
    "            score += 0.5\n",
    "        return score\n",
    "\n",
    "    candidates: list[tuple[str, TavilySearchOutput, float]] = []\n",
    "    seen_urls: set[str] = set()\n",
    "\n",
    "    # Collect per-site, compute scores\n",
    "    for site, q in site_queries.items():\n",
    "        try:\n",
    "            r = tavily_client.search(query=q, search_depth=\"basic\", max_results=10)\n",
    "        except Exception:\n",
    "            continue\n",
    "        if not isinstance(r, dict) or \"results\" not in r:\n",
    "            continue\n",
    "        for h in r[\"results\"]:\n",
    "            url = h.get(\"url\", \"\") if isinstance(h, dict) else \"\"\n",
    "            site_name = which_site(url) if url else None\n",
    "            if site_name is None:\n",
    "                continue\n",
    "            if url in seen_urls:\n",
    "                continue\n",
    "            seen_urls.add(url)\n",
    "            title = h.get(\"title\", \"\")\n",
    "            content = h.get(\"content\", \"\")\n",
    "            tso = TavilySearchOutput(title=title, url=url, content=content)\n",
    "            score = relevance_score(site_name, title, url, content)\n",
    "            candidates.append((site_name, tso, score))\n",
    "\n",
    "    # Sort globally by relevance (desc)\n",
    "    candidates.sort(key=lambda x: x[2], reverse=True)\n",
    "\n",
    "    # Seed with top-1 per site if available to ensure a mix\n",
    "    selected: list[TavilySearchOutput] = []\n",
    "    used_urls: set[str] = set()\n",
    "    sites_present = {s for s, _, _ in candidates}\n",
    "    for s in (\"kaggle\", \"huggingface\"):\n",
    "        if s in sites_present:\n",
    "            for site_name, tso, _ in candidates:\n",
    "                if site_name == s and tso.url not in used_urls:\n",
    "                    selected.append(tso)\n",
    "                    used_urls.add(tso.url)\n",
    "                    break\n",
    "        if len(selected) >= 2:\n",
    "            break\n",
    "\n",
    "    # Fill remaining slots by overall relevance\n",
    "    for _, tso, _ in candidates:\n",
    "        if len(selected) >= 10:\n",
    "            break\n",
    "        if tso.url in used_urls:\n",
    "            continue\n",
    "        selected.append(tso)\n",
    "        used_urls.add(tso.url)\n",
    "\n",
    "    # Cap to 10\n",
    "    selected = selected[:10]\n",
    "\n",
    "    return {\"tavilySearchOutput\": selected}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "c7fed2c5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'tavilySearchOutput': [TavilySearchOutput(title='COVID-19 Dataset - Kaggle', url='https://www.kaggle.com/datasets/imdevskp/corona-virus-report', content=\"# COVID-19 Dataset ## COVID-19 Dataset ## About Dataset * As of 30 January 2020, approximately 8,243 cases have been confirmed > * **full\\\\_grouped.csv** - Day to day country wise no. of cases (Has County/State/Province level data) > * **covid\\\\_19\\\\_clean\\\\_complete.csv** - Day to day country wise no. of cases (Doesn't have County/State/Province level data) > * **country\\\\_wise\\\\_latest.csv** - Latest country level no. > * **day\\\\_wise.csv** - Day wise no. of cases (Doesn't have country level data) > * **usa\\\\_county\\\\_wise.csv** - Day to day county level no. > * **worldometer\\\\_data.csv** - Latest data from  ### Similar Datasets ## See what others are saying about this dataset ### What have you used this dataset for? ### How would you describe this dataset?\"),\n",
       "  TavilySearchOutput(title='xhluca/publichealth-qa · Datasets at Hugging Face', url='https://huggingface.co/datasets/xhluca/publichealth-qa', content='COVID-19 is a new disease, caused be a novel (or new) coronavirus that has not previously been seen in humans. The name of this disease was selected following'),\n",
       "  TavilySearchOutput(title='Chest X-ray (Covid-19 & Pneumonia) - Kaggle', url='https://www.kaggle.com/datasets/prashant268/chest-xray-covid19-pneumonia', content='Image 9 X-ray Image Classification using Transfer Learning more_vert Updated 2 years ago Chest X-ray (Covid-19 & Pneumonia) Image 11 COVID19_Pneumonia_Normal_Chest_Xray_PA_Dataset more_vert [Amanullah Asraf · Updated 5 years ago Usability 8.1 · 2 GB · 3,632 downloads](https://www.kaggle.com/datasets/amanullahasraf/covid19-pneumonia-normal-chest-xray-pa-dataset) Image 13 COVID-19 & Normal Posteroanterior(PA) X-rays more_vert [Tarandeep Singh · Updated 5 years ago Usability 8.8 · 137 MB · 429 downloads 280 Files (other)](https://www.kaggle.com/datasets/tarandeep97/covid19-normal-posteroanteriorpa-xrays) Image 14 Chest X-ray Image(COVID19, PNEUMONIA, and NORMAL) more_vert [Al Sani · Updated 2 years ago Usability 7.5 · 2 GB · 1,296 downloads 6432 Files (other)](https://www.kaggle.com/datasets/alsaniipe/chest-x-ray-image) Image 16 COVID Chest X-Ray Image Dataset more_vert [Latiful Joy · Updated 3 years ago Usability 5.0 · 507 MB · 102 downloads 1608 Files (other)](https://www.kaggle.com/datasets/latifuljoy/covid-chest-xray-image-dataset)'),\n",
       "  TavilySearchOutput(title='Real-time Covid 19 Data - Kaggle', url='https://www.kaggle.com/datasets/gauravduttakiit/covid-19/versions/775/data', content='This dataset includes time series data tracking the number of people affected by COVID-19 worldwide, including: - confirmed tested cases of Coronavirus'),\n",
       "  TavilySearchOutput(title='COVID-19 Open Research Dataset Challenge (CORD-19) - Kaggle', url='https://www.kaggle.com/datasets/allen-institute-for-ai/CORD-19-research-challenge', content='# COVID-19 Open Research Dataset Challenge (CORD-19) ## COVID-19 Open Research Dataset Challenge (CORD-19) ## About Dataset This freely available dataset is provided to the global research community to apply recent advances in natural language processing and other AI techniques to generate new insights in support of the ongoing fight against this infectious disease. This allows the worldwide AI research community the opportunity to apply text and data mining approaches to find answers to questions within, and connect insights across, this content in support of the ongoing COVID-19 response efforts worldwide. ### What have you used this dataset for? ### How would you describe this dataset?'),\n",
       "  TavilySearchOutput(title='COVID-19 Symptoms Checker - Kaggle', url='https://www.kaggle.com/datasets/iamhungundji/covid19-symptoms-checker', content='These data will help to identify whether any person is having a coronavirus disease or not based on some pre-defined standard symptoms.'),\n",
       "  TavilySearchOutput(title='Coronavirus News (COVID-19) - Kaggle', url='https://www.kaggle.com/datasets/saadaljebrreen/corona-various-news-covid19', content='A novel coronavirus (COVID-19) was identified in 2019 in Wuhan, China. This is a new coronavirus that has not been previously identified in humans.'),\n",
       "  TavilySearchOutput(title='COVID-19 in USA - Kaggle', url='https://www.kaggle.com/datasets/sudalairajkumar/covid19-in-usa', content='The number of new cases are increasing day by day around the world. This dataset has information from 50 US states and the District of Columbia at daily level.'),\n",
       "  TavilySearchOutput(title='COVID-19 & the virus that causes it: SARS-CoV-2. - Kaggle', url='https://www.kaggle.com/datasets/patricklford/covid-19', content='About Dataset · Total COVID Cases Over Time: This plot shows a cumulative increase in the number of confirmed COVID-19 cases reported globally since the'),\n",
       "  TavilySearchOutput(title='COVID-19 Radiography Database - Kaggle', url='https://www.kaggle.com/datasets/tawsifurrahman/covid19-radiography-database', content='A database of chest X-ray images for COVID-19 positive cases along with Normal and Viral Pneumonia images.')]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "state = AgentState(query=\"Coronvirus diseases\")\n",
    "\n",
    "tavily_search_tool(state)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dca55ff8",
   "metadata": {},
   "source": [
    "# Get datasest from Kaggle"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "146ff5ca",
   "metadata": {},
   "source": [
    "## Tool to download Kaggle Files: download_kaggle_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbb6027f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset URL: https://www.kaggle.com/datasets/sudalairajkumar/novel-corona-virus-2019-dataset\n",
      "{'csv': ['./data/time_series_covid_19_confirmed_US.csv', './data/time_series_covid_19_recovered.csv', './data/time_series_covid_19_deaths_US.csv', './data/covid_19_data.csv', './data/time_series_covid_19_deaths.csv', './data/time_series_covid_19_confirmed.csv'], 'excel': []}\n"
     ]
    }
   ],
   "source": [
    "from kaggle.api.kaggle_api_extended import KaggleApi\n",
    "import os\n",
    "import re\n",
    "\n",
    "def download_kaggle_files(url: str, data_dir: str = './data') -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Given a Kaggle dataset URL, downloads the dataset using the Kaggle API,\n",
    "    unzips it, and returns a dictionary with lists of CSV and Excel file paths extracted.\n",
    "\n",
    "    Args:\n",
    "        url (str): The Kaggle dataset URL, e.g. 'https://www.kaggle.com/datasets/sudalairajkumar/novel-corona-virus-2019-dataset' or 'https://www.kaggle.com/datasets/einsteindata4u/covid19'\n",
    "        data_dir (str): Directory to download and extract files to.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, List[str]]: Dictionary with keys 'csv' and 'excel', each mapping to a list of file paths.\n",
    "    \"\"\"\n",
    "    # Extract dataset slug from URL\n",
    "    m = re.search(r'kaggle\\.com/datasets/([^/]+/[^/?#]+)', url)\n",
    "    if not m:\n",
    "        raise ValueError(f\"Could not parse Kaggle dataset slug from URL: {url}\")\n",
    "    dataset_slug = m.group(1)\n",
    "\n",
    "    api = KaggleApi()\n",
    "    api.authenticate()\n",
    "\n",
    "    os.makedirs(data_dir, exist_ok=True)\n",
    "    api.dataset_download_files(\n",
    "        dataset_slug,\n",
    "        path=data_dir,\n",
    "        force=True,\n",
    "        quiet=True,\n",
    "        unzip=True\n",
    "    )\n",
    "\n",
    "    # Find all CSV and Excel files in the data_dir\n",
    "    csv_files = []\n",
    "    excel_files = []\n",
    "    for root, _, files in os.walk(data_dir):\n",
    "        for f in files:\n",
    "            if f.lower().endswith('.csv'):\n",
    "                csv_files.append(os.path.join(root, f))\n",
    "            elif f.lower().endswith('.xlsx') or f.lower().endswith('.xls'):\n",
    "                excel_files.append(os.path.join(root, f))\n",
    "    return {\"csv\": csv_files, \"excel\": excel_files}\n",
    "\n",
    "# Example usage:\n",
    "tabular_files = download_kaggle_files('https://www.kaggle.com/datasets/sudalairajkumar/novel-corona-virus-2019-dataset')\n",
    "print(tabular_files)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df49e03b",
   "metadata": {},
   "source": [
    "## Tool to download Hugging Face Files : download_huggingface_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8cc97e72",
   "metadata": {},
   "outputs": [],
   "source": [
    "from huggingface_hub import snapshot_download, hf_hub_download\n",
    "from urllib.parse import urlparse\n",
    "from pathlib import Path\n",
    "import shutil\n",
    "\n",
    "\n",
    "def _parse_hf_dataset_url(url: str):\n",
    "    \"\"\"\n",
    "    Parse a Hugging Face dataset URL and return (repo_id, revision, subpath, is_resolve_file).\n",
    "    Supports forms like:\n",
    "      - https://huggingface.co/datasets/<owner>/<name>\n",
    "      - https://huggingface.co/datasets/<owner>/<name>/tree/<rev>/<subdir...>\n",
    "      - https://huggingface.co/datasets/<owner>/<name>/resolve/<rev>/<path/to/file>\n",
    "    \"\"\"\n",
    "    p = urlparse(url)\n",
    "    parts = [s for s in p.path.split('/') if s]\n",
    "    if not parts or parts[0] != 'datasets' or len(parts) < 3:\n",
    "        raise ValueError(f\"Not a datasets repo URL: {url}\")\n",
    "\n",
    "    owner, name = parts[1], parts[2]\n",
    "    repo_id = f\"{owner}/{name}\"\n",
    "    revision = 'main'\n",
    "    subpath = None\n",
    "    is_resolve = False\n",
    "\n",
    "    if 'resolve' in parts:\n",
    "        i = parts.index('resolve')\n",
    "        if i + 1 < len(parts):\n",
    "            revision = parts[i + 1]\n",
    "        if i + 2 < len(parts):\n",
    "            subpath = '/'.join(parts[i + 2:])\n",
    "        is_resolve = True\n",
    "    elif 'tree' in parts:\n",
    "        i = parts.index('tree')\n",
    "        if i + 1 < len(parts):\n",
    "            revision = parts[i + 1]\n",
    "        if i + 2 < len(parts):\n",
    "            subpath = '/'.join(parts[i + 2:])\n",
    "\n",
    "    return repo_id, revision, subpath, is_resolve\n",
    "\n",
    "\n",
    "def download_huggingface_files(url: str, data_dir: str = './data') -> dict[str, list[str]]:\n",
    "    \"\"\"\n",
    "    Given a Hugging Face dataset URL, download CSV/XLS/XLSX files into data_dir and\n",
    "    return a dict with lists of saved file paths: {'csv': [...], 'excel': [...]}.\n",
    "    \"\"\"\n",
    "    repo_id, revision, subpath, is_resolve = _parse_hf_dataset_url(url)\n",
    "\n",
    "    out_dir = Path(data_dir)\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    csv_files: list[str] = []\n",
    "    excel_files: list[str] = []\n",
    "\n",
    "    def _record(path: Path):\n",
    "        if path.suffix.lower() == '.csv':\n",
    "            csv_files.append(str(path))\n",
    "        elif path.suffix.lower() in ('.xlsx', '.xls'):\n",
    "            excel_files.append(str(path))\n",
    "\n",
    "    # Direct file via /resolve\n",
    "    if is_resolve and subpath:\n",
    "        if Path(subpath).suffix.lower() == '.csv':\n",
    "            local_path = hf_hub_download(repo_id=repo_id, filename=subpath, revision=revision, repo_type='dataset')\n",
    "            dest = out_dir / Path(subpath).name\n",
    "            shutil.copy(local_path, dest)\n",
    "            _record(dest)\n",
    "            return {\"csv\": csv_files, \"excel\": excel_files}\n",
    "        return {\"csv\": csv_files, \"excel\": excel_files}\n",
    "\n",
    "    # Snapshot the repository and optionally restrict to subdir\n",
    "    local_dir = Path(snapshot_download(repo_id=repo_id, repo_type='dataset', revision=revision))\n",
    "    base = local_dir / subpath if subpath else local_dir\n",
    "\n",
    "    if not base.exists():\n",
    "        return {\"csv\": csv_files, \"excel\": excel_files}\n",
    "\n",
    "    for p in base.rglob('*'):\n",
    "        if p.is_file() and p.suffix.lower() == '.csv':\n",
    "            dest = out_dir / p.name\n",
    "            try:\n",
    "                shutil.copy(p, dest)\n",
    "                _record(dest)\n",
    "            except Exception:\n",
    "                pass\n",
    "\n",
    "    return {\"csv\": csv_files, \"excel\": excel_files}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "06d84de5",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Fetching 10 files: 100%|██████████| 10/10 [00:00<00:00, 11.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'csv': ['data/english.csv', 'data/spanish.csv', 'data/korean.csv', 'data/arabic.csv', 'data/chinese.csv', 'data/vietnamese.csv', 'data/russian.csv', 'data/french.csv'], 'excel': []}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "files = download_huggingface_files('https://huggingface.co/datasets/xhluca/publichealth-qa')\n",
    "print(files)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
